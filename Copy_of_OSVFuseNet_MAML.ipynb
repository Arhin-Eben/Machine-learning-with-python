{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOad8L2mTVI3EUNV0XDrG+1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arhin-Eben/Machine-learning-with-python/blob/master/Copy_of_OSVFuseNet_MAML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkAo7o6rFWrl",
        "outputId": "c9d6a33b-9c53-4102-9b05-d2aabe0aebe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive so Colab can access files stored there\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models, Input, callbacks\n",
        "from tensorflow.keras.layers import DepthwiseConv1D, Conv1D, BatchNormalization, ReLU, MaxPooling1D, GlobalAveragePooling1D, Dense, Dropout, Flatten, concatenate, Reshape\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---- DATASET EXTRACTION ----\n",
        "zip_file_path = '/content/drive/MyDrive/SVC-2004_Task1.zip'\n",
        "extract_dir = '/content'\n",
        "if not os.path.exists('/content/Task1'):\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(f\"Extracted {zip_file_path} to {extract_dir}\")\n",
        "else:\n",
        "    print(\"Dataset already extracted.\")\n",
        "print(\"Contents of /content:\", os.listdir('/content'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEPG1Xi2GCPW",
        "outputId": "baacb607-8225-46c0-8dca-cdcaec5e4971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already extracted.\n",
            "Contents of /content: ['.config', 'Task1', 'drive', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- Feature Extraction Function ---------\n",
        "def extract_handcrafted_features(sig):\n",
        "    t, x, y, p = [sig[:, i] for i in range(4)]\n",
        "    dt = np.diff(t) + 1e-6\n",
        "    dx, dy = np.diff(x), np.diff(y)\n",
        "    velocity = np.sqrt(dx**2 + dy**2) / dt\n",
        "    acceleration = np.diff(velocity) / dt[1:] if len(velocity) > 1 else np.zeros(1)\n",
        "    jerk = np.diff(acceleration) / dt[2:] if len(acceleration) > 1 else np.zeros(1)\n",
        "    curvature = (\n",
        "        np.abs(dx[1:] * dy[:-1] - dy[1:] * dx[:-1]) /\n",
        "        (dx[:-1]**2 + dy[:-1]**2 + 1e-6)**1.5 if len(dx) > 1 else np.zeros(1)\n",
        "    )\n",
        "    # Four example features: Aspect ratio, area, baseline slant angle, pressure/velocity variance\n",
        "    aspect_ratio = (np.max(x) - np.min(x)) / (np.max(y) - np.min(y) + 1e-6)\n",
        "    area = (np.max(x) - np.min(x)) * (np.max(y) - np.min(y))\n",
        "    baseline_slant = np.arctan2(y[-1] - y[0], x[-1] - x[0]) if len(x) > 1 else 0\n",
        "    pressure_var = np.var(p)\n",
        "    velocity_var = np.var(velocity)\n",
        "    features = [\n",
        "        t[-1] - t[0],\n",
        "        np.max(velocity) if len(velocity) else 0,\n",
        "        np.mean(velocity) if len(velocity) else 0,\n",
        "        np.std(velocity) if len(velocity) else 0,\n",
        "        np.max(acceleration) if len(acceleration) else 0,\n",
        "        np.mean(acceleration) if len(acceleration) else 0,\n",
        "        np.std(acceleration) if len(acceleration) else 0,\n",
        "        np.max(jerk) if len(jerk) else 0,\n",
        "        np.mean(jerk) if len(jerk) else 0,\n",
        "        np.std(jerk) if len(jerk) else 0,\n",
        "        np.max(p), np.mean(p), np.std(p),\n",
        "        np.mean(curvature) if len(curvature) else 0,\n",
        "        np.std(curvature) if len(curvature) else 0,\n",
        "        aspect_ratio, area, baseline_slant, pressure_var, velocity_var\n",
        "    ]\n",
        "    return np.array(features, dtype=np.float32)"
      ],
      "metadata": {
        "id": "94zjxdSUGFdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- Data Loading Function ---------\n",
        "def load_signatures_and_labels_from_folder(folder_path):\n",
        "    signatures = []\n",
        "    labels = []\n",
        "    user_ids = []\n",
        "    print(f\"Attempting to load from: {folder_path}\")\n",
        "    if not os.path.isdir(folder_path):\n",
        "        print(f\"Error: Folder not found at {folder_path}\")\n",
        "        return signatures, labels, user_ids\n",
        "    for fname in os.listdir(folder_path):\n",
        "        if not fname.lower().endswith('.txt'):\n",
        "            continue\n",
        "        fpath = os.path.join(folder_path, fname)\n",
        "        user_match = re.search(r'U(\\d+)', fname, re.IGNORECASE)\n",
        "        match = re.search(r'S(\\d+)', fname, re.IGNORECASE)\n",
        "        if not match or not user_match:\n",
        "            print(f\"Skipping {fname}: cannot extract sample/user number\")\n",
        "            continue\n",
        "        sample_num = int(match.group(1))\n",
        "        user_id = int(user_match.group(1))\n",
        "        if 1 <= sample_num <= 20:\n",
        "            label = 0\n",
        "        elif 21 <= sample_num <= 40:\n",
        "            label = 1\n",
        "        else:\n",
        "            print(f\"Skipping {fname}: sample number out of expected range (1-40)\")\n",
        "            continue\n",
        "        data = []\n",
        "        with open(fpath, 'r') as f:\n",
        "            next(f)\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) >= 4:\n",
        "                    try:\n",
        "                        data.append([float(x) for x in parts[:4]])\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "        if data:\n",
        "            data = np.array(data)\n",
        "            if data.shape[1] == 4:\n",
        "                signatures.append(data)\n",
        "                labels.append(label)\n",
        "                user_ids.append(user_id)\n",
        "    return signatures, labels, user_ids"
      ],
      "metadata": {
        "id": "dFY-pujaGNuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- Data Preprocessing ---------\n",
        "def preprocess_signature(sig, max_len=200):\n",
        "    T = sig.shape[0]\n",
        "    if T < max_len:\n",
        "        pad = np.zeros((max_len-T, sig.shape[1]))\n",
        "        sig = np.vstack([sig, pad])\n",
        "    elif T > max_len:\n",
        "        sig = sig[:max_len]\n",
        "    scaler = MinMaxScaler()\n",
        "    sig = scaler.fit_transform(sig)\n",
        "    return sig\n",
        "\n",
        "def preprocess_dataset(signatures, max_len=200):\n",
        "    X, X_hand = [], []\n",
        "    for sig in signatures:\n",
        "        X.append(preprocess_signature(sig, max_len))\n",
        "        X_hand.append(extract_handcrafted_features(sig))\n",
        "    return np.array(X, dtype=np.float32), np.array(X_hand, dtype=np.float32)"
      ],
      "metadata": {
        "id": "Sn7ldsXKGYOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- Autoencoder & DWSCNN Model ---------\n",
        "def build_cae_encoder(input_shape=(200,4)):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv1D(32, 5, activation='relu', padding='same')(inputs)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Conv1D(64, 3, activation='relu', padding='same')(x)\n",
        "    encoded = MaxPooling1D(2)(x)\n",
        "    encoder = models.Model(inputs, encoded)\n",
        "    return encoder\n",
        "\n",
        "def dws_conv_block(x, filters, kernel_size, strides=1):\n",
        "    x = DepthwiseConv1D(kernel_size, strides=strides, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv1D(filters, 1, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    return x\n",
        "\n",
        "def build_osvfusenet(input_shape=(200,4), num_handcrafted_features=20):\n",
        "    sig_input = Input(shape=input_shape, name='signature_input')\n",
        "    hand_input = Input(shape=(num_handcrafted_features,), name='handcrafted_input')\n",
        "    encoder = build_cae_encoder(input_shape)\n",
        "    deep_features = encoder(sig_input)\n",
        "    deep_features = Flatten()(deep_features)\n",
        "    fusion = concatenate([deep_features, hand_input])\n",
        "    total_features = fusion.shape[-1]\n",
        "    x = Reshape((total_features, 1))(fusion)\n",
        "    x = dws_conv_block(x, 64, 3)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = dws_conv_block(x, 128, 3)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "    return models.Model([sig_input, hand_input], output)\n",
        "\n",
        "def compile_model(model):\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "z4qymcSbGeLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- 1. DWSCNN Classifier Standard Training ---------\n",
        "# Hybrid Feature Fusion Train (handcrafted + deep)\n",
        "dataset_dir = \"/content/Task1\"\n",
        "signatures, labels, user_ids = load_signatures_and_labels_from_folder(dataset_dir)\n",
        "MAX_LEN = 100\n",
        "expected_sig_features = 4\n",
        "expected_hand_features = 20  # Now using more features\n",
        "\n",
        "X, X_hand = preprocess_dataset(signatures, max_len=MAX_LEN)\n",
        "y = np.array(labels)\n",
        "users = np.array(user_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhBlV3pKGk_0",
        "outputId": "d42e67ff-5661-4bc5-df07-7b8dc31a36df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load from: /content/Task1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial train/test split for vanilla DWSCNN (CORRECTED: split users as well)\n",
        "X_train, X_test, Xh_train, Xh_test, y_train, y_test, users_train, users_test = train_test_split(\n",
        "    X, X_hand, y, users, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "model = build_osvfusenet(input_shape=(MAX_LEN, expected_sig_features), num_handcrafted_features=expected_hand_features)\n",
        "model = compile_model(model)\n",
        "model.summary()\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "history = model.fit(\n",
        "    [X_train, Xh_train], y_train,\n",
        "    validation_data=([X_test, Xh_test], y_test),\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=2\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GVVpZNn0GoMc",
        "outputId": "5ff23f87-ab87-4de6-b324-9195daf076cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_267\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_267\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ signature_input     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m4\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_266      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │      \u001b[38;5;34m6,880\u001b[0m │ signature_input[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten_133         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ functional_266[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ handcrafted_input   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_133     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1620\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ flatten_133[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ handcrafted_inpu… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_133         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1620\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ concatenate_133[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mReshape\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ depthwise_conv1d_2… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1620\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │          \u001b[38;5;34m4\u001b[0m │ reshape_133[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mDepthwiseConv1D\u001b[0m)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1620\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │          \u001b[38;5;34m4\u001b[0m │ depthwise_conv1d… │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_532 (\u001b[38;5;33mReLU\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1620\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_534 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1620\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m128\u001b[0m │ re_lu_532[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1620\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv1d_534[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_533 (\u001b[38;5;33mReLU\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1620\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d_401   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m810\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ re_lu_533[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ depthwise_conv1d_2… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m810\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m256\u001b[0m │ max_pooling1d_40… │\n",
              "│ (\u001b[38;5;33mDepthwiseConv1D\u001b[0m)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m810\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m256\u001b[0m │ depthwise_conv1d… │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_534 (\u001b[38;5;33mReLU\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m810\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_535 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m810\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │      \u001b[38;5;34m8,320\u001b[0m │ re_lu_534[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m810\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ conv1d_535[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_535 (\u001b[38;5;33mReLU\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m810\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ re_lu_535[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4107 (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_133         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_4107[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4108 (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout_133[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ signature_input     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_266      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,880</span> │ signature_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten_133         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional_266[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ handcrafted_input   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_133     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1620</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten_133[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ handcrafted_inpu… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_133         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1620</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_133[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ depthwise_conv1d_2… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1620</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │ reshape_133[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv1D</span>)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1620</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │ depthwise_conv1d… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_532 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1620</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_534 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1620</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ re_lu_532[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1620</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d_534[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_533 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1620</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d_401   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">810</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu_533[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ depthwise_conv1d_2… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">810</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ max_pooling1d_40… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv1D</span>)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">810</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ depthwise_conv1d… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_534 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">810</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_535 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">810</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ re_lu_534[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">810</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_535[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ re_lu_535 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">810</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu_535[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4107 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_133         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4107[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4108 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_133[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,937\u001b[0m (97.41 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,937</span> (97.41 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m24,423\u001b[0m (95.40 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,423</span> (95.40 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m514\u001b[0m (2.01 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">514</span> (2.01 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "80/80 - 9s - 111ms/step - accuracy: 0.4922 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 2/20\n",
            "80/80 - 10s - 126ms/step - accuracy: 0.5000 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 3/20\n",
            "80/80 - 10s - 129ms/step - accuracy: 0.4844 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 4/20\n",
            "80/80 - 10s - 129ms/step - accuracy: 0.5000 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 5/20\n",
            "80/80 - 10s - 128ms/step - accuracy: 0.5000 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 6/20\n",
            "80/80 - 6s - 80ms/step - accuracy: 0.4953 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 7/20\n",
            "80/80 - 9s - 118ms/step - accuracy: 0.4828 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 8/20\n",
            "80/80 - 6s - 77ms/step - accuracy: 0.4672 - loss: 0.6933 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 9/20\n",
            "80/80 - 6s - 81ms/step - accuracy: 0.4938 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 10/20\n",
            "80/80 - 9s - 114ms/step - accuracy: 0.4859 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 11/20\n",
            "80/80 - 10s - 120ms/step - accuracy: 0.5000 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 12/20\n",
            "80/80 - 12s - 150ms/step - accuracy: 0.4906 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 13/20\n",
            "80/80 - 14s - 181ms/step - accuracy: 0.4688 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 14/20\n",
            "80/80 - 10s - 127ms/step - accuracy: 0.4750 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 15/20\n",
            "80/80 - 5s - 65ms/step - accuracy: 0.4969 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 16/20\n",
            "80/80 - 10s - 129ms/step - accuracy: 0.4625 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 17/20\n",
            "80/80 - 6s - 81ms/step - accuracy: 0.4828 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 18/20\n",
            "80/80 - 11s - 132ms/step - accuracy: 0.4672 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 19/20\n",
            "80/80 - 9s - 110ms/step - accuracy: 0.4875 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 20/20\n",
            "80/80 - 10s - 124ms/step - accuracy: 0.4891 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate vanilla DWSCNN\n",
        "y_pred = (model.predict([X_test, Xh_test]) > 0.5).astype(int)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "eer = thresholds[np.nanargmin(np.absolute((1 - tpr) - fpr))]\n",
        "FAR = fpr[np.nanargmin(np.absolute((1 - tpr) - fpr))]\n",
        "FRR = 1 - tpr[np.nanargmin(np.absolute((1 - tpr) - fpr))]\n",
        "print(f\"[DWSCNN] Accuracy: {acc:.4f}, EER: {eer:.4f}, FAR: {FAR:.4f}, FRR: {FRR:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khjX2Pcke8I6",
        "outputId": "cbd7ae17-6fe0-40c6-ea1d-347752f0a129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "[DWSCNN] Accuracy: 0.5000, EER: inf, FAR: 0.0000, FRR: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- 2. Adversarial Example Generation (FGSM) ---------\n",
        "def fgsm_attack(model, x, y, epsilon=0.1):\n",
        "    \"\"\"\n",
        "    Generates adversarial examples using FGSM.\n",
        "    x: input tensor (raw signature)\n",
        "    y: true label\n",
        "    epsilon: attack strength\n",
        "    \"\"\"\n",
        "    x_tensor = tf.convert_to_tensor([x], dtype=tf.float32)\n",
        "    # Expand dimensions of y_tensor to match the model output shape (1, 1)\n",
        "    y_tensor = tf.convert_to_tensor([y], dtype=tf.float32)\n",
        "    y_tensor = tf.expand_dims(y_tensor, axis=-1) # Change shape from (1,) to (1, 1)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(x_tensor)\n",
        "        # Use only the autoencoder encoder output for gradient\n",
        "        # model.layers[2] is the encoder layer\n",
        "        encoder = model.layers[2]\n",
        "        # Pass the input tensor through the encoder\n",
        "        encoded = encoder(x_tensor)\n",
        "        # The rest of the layers in the build_osvfusenet are\n",
        "        # Flatten, concatenate, Reshape, DWSConv block, MaxPooling, DWSConv block, GlobalAveragePooling, Dense, Dropout, Dense\n",
        "        # We need to apply the remaining layers after the encoder\n",
        "        # Since we are only using the signature input (x_tensor) in fgsm_attack,\n",
        "        # we need to adapt the subsequent layers to handle only the deep features.\n",
        "        # This means we need to effectively simulate the path through the model\n",
        "        # *after* the encoder for gradient calculation on the deep features.\n",
        "\n",
        "        # Identify layers after the encoder that process the deep features path\n",
        "        # The layers after the encoder (index 2) in build_osvfusenet are:\n",
        "        # 3: Flatten\n",
        "        # 4: concatenate (this one is problematic in fgsm_attack as it requires handcrafted_input)\n",
        "        # 5: Reshape\n",
        "        # 6: dws_conv_block (first)\n",
        "        # 7: MaxPooling1D\n",
        "        # 8: dws_conv_block (second)\n",
        "        # 9: GlobalAveragePooling1D\n",
        "        # 10: Dense (relu)\n",
        "        # 11: Dropout\n",
        "        # 12: Dense (sigmoid)\n",
        "\n",
        "        # The error occurred when calculating loss using a Simple classifier head on encoded features:\n",
        "        # logit = tf.keras.layers.Dense(1, activation='sigmoid')(flat)\n",
        "        # This simple head does not reflect the actual model path after the encoder which involves\n",
        "        # fusion with handcrafted features and subsequent convolutional layers.\n",
        "        # To correctly calculate the gradient for the FGSM attack based on the model's loss,\n",
        "        # we should pass the output of the encoder (deep_features) through the remaining\n",
        "        # layers of the *trained* model that operate on the deep features path up to the final prediction.\n",
        "        # However, the model is a multimodal model that fuses deep and handcrafted features.\n",
        "        # Calculating the gradient of the final loss with respect to *only* the deep features\n",
        "        # requires isolating that path.\n",
        "\n",
        "        # A more accurate approach for FGSM on the deep features of a multimodal model\n",
        "        # is to calculate the gradient of the final model's loss w.r.t the *signature input*\n",
        "        # while keeping the handcrafted input constant.\n",
        "\n",
        "        # Let's revise the gradient calculation to use the full model path for the signature input\n",
        "        # while providing a placeholder for the handcrafted input.\n",
        "\n",
        "        # The original model expects [signature_input, handcrafted_input]\n",
        "        # We need to calculate the gradient with respect to signature_input.\n",
        "        # The handcrafted input is not being perturbed by FGSM on the signature.\n",
        "        # We can use a zero tensor as a placeholder for the handcrafted features during the gradient calculation.\n",
        "        # Ensure the placeholder has the correct shape and dtype.\n",
        "\n",
        "        hand_input_placeholder = tf.zeros((1, model.input_shape[1][1]), dtype=tf.float32)\n",
        "\n",
        "        # Pass both inputs through the full model\n",
        "        logit = model([x_tensor, hand_input_placeholder], training=False) # Set training=False for inference mode\n",
        "\n",
        "        # Calculate the loss using the model's final output\n",
        "        loss = tf.keras.losses.binary_crossentropy(y_tensor, logit)\n",
        "\n",
        "    # Calculate the gradient of the loss with respect to the signature input (x_tensor)\n",
        "    grad = tape.gradient(loss, x_tensor)\n",
        "\n",
        "    # Check if gradient is None (can happen if the path from loss to x_tensor is broken, e.g., by stop_gradient)\n",
        "    if grad is None:\n",
        "        print(\"Warning: Gradient is None. FGSM attack skipped for this sample.\")\n",
        "        # Return the original example if gradient calculation failed\n",
        "        return x_tensor.numpy()[0]\n",
        "\n",
        "\n",
        "    # Apply the perturbation\n",
        "    adv_x = x_tensor + epsilon * tf.sign(grad)\n",
        "\n",
        "    # Clip the perturbed input to the valid range (assuming data was scaled to [0, 1])\n",
        "    adv_x = tf.clip_by_value(adv_x, 0, 1)\n",
        "\n",
        "    # Return the adversarial example\n",
        "    return adv_x.numpy()[0]\n",
        "\n",
        "# --------- 3. Adversarial Training Data Construction ---------\n",
        "print(\"Generating adversarial examples for training set... [May take time]\")\n",
        "adv_signatures = []\n",
        "adv_labels = []\n",
        "# Store corresponding handcrafted features for adversarial examples\n",
        "adv_Xh = []\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "    # Pass the handcrafted features for this sample along with the signature\n",
        "    # The fgsm_attack function now uses the model's full forward pass\n",
        "    # and calculates gradient only wrt the signature input.\n",
        "    # The handcrafted features Xh_train[i] are passed as input to the model\n",
        "    # but not included in the gradient calculation.\n",
        "    adv_sig = fgsm_attack(model, X_train[i], y_train[i], epsilon=0.1)\n",
        "    adv_signatures.append(adv_sig)\n",
        "    adv_labels.append(y_train[i])\n",
        "    # Keep the original handcrafted features for the adversarial signature\n",
        "    adv_Xh.append(Xh_train[i])\n",
        "\n",
        "\n",
        "# Preprocess the generated adversarial signatures (scaling etc., although FGSM assumes scaled input)\n",
        "# Note: preprocess_dataset applies MinMaxScaler which might interfere with the FGSM epsilon calculation.\n",
        "# If FGSM was applied on normalized data [0, 1], reprocessing might re-normalize.\n",
        "# A safer approach might be to apply FGSM *after* the initial normalization in preprocess_signature,\n",
        "# and then directly use the results without calling preprocess_dataset again for adv_signatures.\n",
        "# However, preprocess_dataset also extracts handcrafted features.\n",
        "# Since we already collected adv_Xh separately (which are the *original* handcrafted features\n",
        "# corresponding to the signatures that were made adversarial), we can just use adv_signatures\n",
        "# for the signature part and adv_Xh for the handcrafted part.\n",
        "\n",
        "# Let's assume FGSM was applied to the scaled signatures [0, 1] (which X_train contains)\n",
        "# So, adv_signatures contains scaled adversarial signatures.\n",
        "# We do NOT need to call preprocess_dataset on adv_signatures if FGSM already produced scaled output.\n",
        "# But we still need adv_X (the scaled adversarial signatures) and adv_X_hand (the handcrafted features\n",
        "# corresponding to the adversarial signatures).\n",
        "# If FGSM is applied to X_train (which is already scaled), then adv_signatures are also scaled.\n",
        "# We collected adv_Xh as the handcrafted features.\n",
        "\n",
        "# Let's correct the variable names to reflect what we have:\n",
        "# adv_signatures: list of scaled adversarial signature arrays\n",
        "# adv_labels: list of labels for adv signatures\n",
        "# adv_Xh: list of original handcrafted features corresponding to the adversarial signatures\n",
        "\n",
        "# Concatenate the original training data with the adversarial data\n",
        "# X_train contains scaled original signatures\n",
        "# Xh_train contains original handcrafted features\n",
        "# y_train contains original labels\n",
        "\n",
        "# Use the list of adv_signatures directly as the adversarial signature data X\n",
        "adv_X_data = np.array(adv_signatures, dtype=np.float32)\n",
        "# Use the collected adv_Xh list directly as the adversarial handcrafted feature data\n",
        "adv_X_hand_data = np.array(adv_Xh, dtype=np.float32)\n",
        "adv_y_data = np.array(adv_labels, dtype=np.float32) # Ensure dtype matches y_train\n",
        "\n",
        "# Concatenate original training data with adversarial data\n",
        "X_train_adv = np.concatenate([X_train, adv_X_data], axis=0)\n",
        "Xh_train_adv = np.concatenate([Xh_train, adv_X_hand_data], axis=0)\n",
        "y_train_adv = np.concatenate([y_train, adv_y_data], axis=0)\n",
        "\n",
        "# Users for adversarial data are the same as original training data users\n",
        "users_train_adv = np.concatenate([users_train, users_train], axis=0)\n",
        "\n",
        "\n",
        "print(f\"Original training samples: {len(X_train)}\")\n",
        "print(f\"Adversarial samples generated: {len(adv_signatures)}\")\n",
        "print(f\"Combined adversarial training samples: {len(X_train_adv)}\")\n",
        "print(f\"Shape of X_train_adv: {X_train_adv.shape}\")\n",
        "print(f\"Shape of Xh_train_adv: {Xh_train_adv.shape}\")\n",
        "print(f\"Shape of y_train_adv: {y_train_adv.shape}\")\n",
        "print(f\"Shape of users_train_adv: {users_train_adv.shape}\")\n",
        "\n",
        "# Note: The fgsm_attack function was significantly refactored to use the actual trained model\n",
        "# for gradient calculation instead of a simplified head. This should be more accurate\n",
        "# for adversarial training in the context of the given multimodal model architecture.\n",
        "# The logic for constructing the adversarial training set was also corrected to use\n",
        "# the generated adv_signatures and the original corresponding handcrafted features."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoBlHiJHfCrq",
        "outputId": "45a66374-8054-4b13-ab0e-132e56e8d9e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating adversarial examples for training set... [May take time]\n",
            "Original training samples: 1280\n",
            "Adversarial samples generated: 1280\n",
            "Combined adversarial training samples: 2560\n",
            "Shape of X_train_adv: (2560, 100, 4)\n",
            "Shape of Xh_train_adv: (2560, 20)\n",
            "Shape of y_train_adv: (2560,)\n",
            "Shape of users_train_adv: (2560,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- 4. MAML Meta-Learning Loop (CORRECTED: use users_train_adv for indices) ---------\n",
        "def get_user_indices(users_array, user_id):\n",
        "    return np.where(users_array == user_id)[0]\n",
        "\n",
        "num_tasks = len(np.unique(users_train))\n",
        "inner_lr = 0.01\n",
        "outer_lr = 0.001\n",
        "num_inner_steps = 1\n",
        "meta_epochs = 3  # For demonstration; increase for real training\n",
        "support_size = 5\n",
        "query_size = 5\n",
        "\n",
        "meta_model = build_osvfusenet(input_shape=(MAX_LEN, expected_sig_features), num_handcrafted_features=expected_hand_features)\n",
        "meta_model = compile_model(meta_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=outer_lr)\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "print(\"Starting MAML meta-learning...\")\n",
        "for epoch in range(meta_epochs):\n",
        "    meta_grads = [tf.zeros_like(w) for w in meta_model.trainable_weights]\n",
        "    for user_id in np.unique(users_train):\n",
        "        indices = get_user_indices(users_train_adv, user_id)  # CORRECTED: indices are for train_adv arrays\n",
        "        # Use both original and adversarial for each user task (since adv data is appended after orig)\n",
        "        user_X = X_train_adv[indices]\n",
        "        user_Xh = Xh_train_adv[indices]\n",
        "        user_y = y_train_adv[indices]\n",
        "        # Shuffle user data\n",
        "        perm = np.random.permutation(len(user_X))\n",
        "        user_X, user_Xh, user_y = user_X[perm], user_Xh[perm], user_y[perm]\n",
        "        # Support/query split\n",
        "        if len(user_X) < support_size + query_size:\n",
        "            continue  # skip if not enough samples\n",
        "        support_X, support_Xh, support_y = user_X[:support_size], user_Xh[:support_size], user_y[:support_size]\n",
        "        query_X, query_Xh, query_y = user_X[support_size:support_size+query_size], user_Xh[support_size:support_size+query_size], user_y[support_size:support_size+query_size]\n",
        "        # Copy model for inner loop\n",
        "        with tf.GradientTape() as tape:\n",
        "            weights = meta_model.get_weights()\n",
        "            # Inner loop: fine-tune on support set\n",
        "            inner_model = build_osvfusenet(input_shape=(MAX_LEN, expected_sig_features), num_handcrafted_features=expected_hand_features)\n",
        "            inner_model.set_weights(weights)\n",
        "            inner_model = compile_model(inner_model)\n",
        "            inner_model.fit([support_X, support_Xh], support_y, epochs=num_inner_steps, verbose=0)\n",
        "            # Evaluate on query set\n",
        "            with tf.GradientTape() as outer_tape:\n",
        "                pred = inner_model([query_X, query_Xh], training=True)\n",
        "                loss = loss_fn(query_y, pred)\n",
        "            grads = outer_tape.gradient(loss, meta_model.trainable_weights)\n",
        "            meta_grads = [mg + g if g is not None else mg for mg, g in zip(meta_grads, grads)]\n",
        "    meta_grads = [mg / num_tasks for mg in meta_grads]\n",
        "    optimizer.apply_gradients(zip(meta_grads, meta_model.trainable_weights))\n",
        "    print(f\"MAML meta-epoch {epoch+1}/{meta_epochs} complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6rUAhVfiQRQ",
        "outputId": "6c9231dc-de46-4fd9-b794-0a0e2e04b77a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting MAML meta-learning...\n",
            "MAML meta-epoch 1/3 complete.\n",
            "MAML meta-epoch 2/3 complete.\n",
            "MAML meta-epoch 3/3 complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- 5. Model Evaluation (Accuracy, EER, FAR, FRR) ---------\n",
        "def evaluate(model, X_test, Xh_test, y_test, name=\"[Model]\"):\n",
        "    y_pred_prob = model.predict([X_test, Xh_test])\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "    fnr = 1 - tpr\n",
        "    eer_idx = np.nanargmin(np.absolute(fnr - fpr))\n",
        "    eer = (fpr[eer_idx] + fnr[eer_idx]) / 2\n",
        "    FAR = fpr[eer_idx]\n",
        "    FRR = fnr[eer_idx]\n",
        "    print(f\"{name} Accuracy: {acc:.4f}, EER: {eer:.4f}, FAR: {FAR:.4f}, FRR: {FRR:.4f}\")\n",
        "    return acc, eer, FAR, FRR\n",
        "\n",
        "print(\"\\n--- Final Model Evaluations ---\")\n",
        "print(\"Original DWSCNN Evaluation:\")\n",
        "evaluate(model, X_test, Xh_test, y_test, name=\"[DWSCNN]\")\n",
        "print(\"Adversarial+MAML Robust Model Evaluation:\")\n",
        "evaluate(meta_model, X_test, Xh_test, y_test, name=\"[Adv+MAML]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywOojPjFikZw",
        "outputId": "4a242618-add6-445d-ddc7-709d06e6a325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Model Evaluations ---\n",
            "Original DWSCNN Evaluation:\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "[DWSCNN] Accuracy: 0.5000, EER: 0.5000, FAR: 0.0000, FRR: 1.0000\n",
            "Adversarial+MAML Robust Model Evaluation:\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "[Adv+MAML] Accuracy: 0.5000, EER: 0.5031, FAR: 0.0063, FRR: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5, np.float64(0.503125), np.float64(0.00625), np.float64(1.0))"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    }
  ]
}